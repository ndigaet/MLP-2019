{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "Para mas detalle sobre el perceptrón simple:\n",
    "\n",
    "https://www.youtube.com/watch?v=5-ROxtrtLbE&t=994s\n",
    "\n",
    "- Vimos varias opciones para hacer crecer la topología \"A lo ancho\": regresión polinomial, SVM.\n",
    "- Las redes neuronales son una alternativa, y se caracterizan por ser modelos que crecen en profundidad.\n",
    "- El elemento constitutivo de una red neuronal, es la neurona, y en general tiene la forma:\n",
    "\n",
    "<img src=\"perceptrón.png\">\n",
    "\n",
    "Este modelo está inspirado en el funcionamiento biológico de una neurona:\n",
    "\n",
    "<img src=\"neuronasf.jpg\">\n",
    "\n",
    "La función de activación simula la activación neuronal. En nuestro caso, esta función debe ser derivable para poder utilizar los métodos de aprendizaje vistos anteriormente.\n",
    "\n",
    "## Función de costo\n",
    "\n",
    "Si la red neuronal será utilizada como un clasificador:\n",
    "- binary crossentropy o softmax crossentropy.\n",
    "\n",
    "Si la red neuronal será utilizada como un regresor:\n",
    "- MSE\n",
    "\n",
    "## Optimizadores:\n",
    "\n",
    "- Todos los vistos anteriormente. Veamos un ejemplo de su aplicación mediante el algoritmo de backpropagation.\n",
    "\n",
    "La demostración es larga pero sencilla, por lo que está en video por si quieren reproducir la explicación de clase.\n",
    "\n",
    "### Algoritmo de backpropagation:\n",
    "\n",
    "https://www.youtube.com/watch?v=zMEUQbN0-l4&t=55s\n",
    "\n",
    "<img src=\"regla_cadena.png\">\n",
    "\n",
    "De esta manera, se pueden aplicar distintas capas (no solo la fully connected) en la medida de que la función de costo de la red sea derivable con respecto a sus parámetros. Tal es el caso de las distintas capas que veremos a lo largo del curso: CNNs, Simple RNN, GRU, LSTM, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Aproximation Theorem\n",
    "\n",
    "Una red neuronal de una sola capa oculta con un número finito de neuronas puede aproximar cualquier función continua en un subconjunto compacto (un intervalo, un rectangulo, etc) de $R^n$\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Universal_approximation_theorem\n",
    "\n",
    "- El teorema no especifica como estimar los parámetros\n",
    "\n",
    "- Se demostró un 1989 para función de activación sigmoidea\n",
    "\n",
    "- En 1991 se demostró que no es sólo valido para la sigmoidea\n",
    "\n",
    "- La función de activación de la salida es siempre lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
